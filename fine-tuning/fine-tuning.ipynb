{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 36px; color: #FFD700\">FINE-TUNING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1`. `Fine-tuning` \n",
    "\n",
    "`Finetune` - điều chỉnh mô hình được đào tạo trước cho một một tác vụ cụ thế với một tạp dữ liệu chuyên biệt nhỏ hơn.\n",
    "- Các tiếp cận này đòi hỏi ít dữ liệu và tính toán hơn so với việc đào tạo mô hình từ đầu\n",
    "\n",
    "=> Tạo thành cách tiếp cận dễ hơn với nhiều người dùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Transformer` cung cấp `Trainer API`, cung cấp bộ tính năng đào tạo toàn diện để finetune bất kỳ mô hình nào trên Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sẽ sử dụng tập `Yelp Review` và xử lý (phân loại `tokenize`, thêm `pad`, cắt `truncate`) cho việc training.\n",
    "- Dùng `map` để preprocessing toàn bộ tập dữ liệu trong một bước"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`tokenizer` - là bộ xử lý văn bản\n",
    "- Chứa các thành phần cố định như \n",
    "    - Vocabulary - ánh xạ từ con (tokens) sang ID số\n",
    "    - `tokenization rules` - quy tắc tách từ\n",
    "    - Các tham số cấu hình xử lý như độ dài tối đa, cách xử lý từ không có trong vocab\n",
    "- tokenizer đóng vai trò `phiên dịch` giữa văn bản thật và `token` - ngôn ngữ mà models hiểu được.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`token` - là đơn vị nhỏ nhất mà mô hình ngôn ngữ có thể xử lý, tương tự như pixel của ảnh, số nguyên tố trong toán.\n",
    "\n",
    "Các loại token phổ biến:\n",
    "- `Word Token`\n",
    "\n",
    "    - `Mỗi từ là 1 token riêng biệt`\n",
    "    - Vd: \"ChatGPT rất mạnh\" => [\"ChatGPT\", \"rất\", \"mạnh\"]\n",
    "\n",
    "- `Subword Token`\n",
    "\n",
    "    - `Tách từ thành các phần nhỏ hơn`\n",
    "    - Vd: \"ChatGPT\" => [\"Chat\", \"G\", \"PT\"]\n",
    "\n",
    "- `Character Token`\n",
    "    \n",
    "    - `Mỗi kí tự là 1 token`\n",
    "    - Vd: \"NLP\" => [\"N\", \"L\", \"P\"]\n",
    "\n",
    "Quá trình token hóa thực tế\n",
    "- Token hóa cơ bản => [\"Tôi\", \"thích\", \"ăn\", \"phở\", \"!\"]\n",
    "- Token hóa từ con (WordPiece) =>  [\"Tôi\", \"thích\", \"ăn\", \"ph\", \"##ở\", \"!\"]\n",
    "- Chuyển thành ID số => [1045, 2214, 2514, 6421, 4789, 999]\n",
    "\n",
    "Đặc điểm kỹ thuật của token\n",
    "- Từ vựng token (Vocabulary): Tập hợp tất cả token mà mô hình biết\n",
    "- Kích thước từ vựng: Thường từ 30k-100k token với mô hình lớn\n",
    "- Ánh xạ 2 chiều: Token ↔ Token ID (số nguyên)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qk/Documents/python/python_programming/pytorch/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 650000/650000 [00:47<00:00, 13583.53 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:03<00:00, 13256.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\") # Tải dataset \"yelp_review_full\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\") \n",
    "# Tải tokenizer đã được huấn luyện trước từ BERT\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128) # Giảm max_length để tránh tràn bộ nhớ\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Giải thích`\n",
    "\n",
    "`tokenize` - thực hiện chuyển đổi văn bản thành các token bằng cách dùng `Hàm tokenizer`\n",
    "- các đoạn văn bản sẽ được cắt ngắn nếu quá dài (`truncation=True`)\n",
    "- (`padding=\"max_length\"`) để đạt được độ dài tối đa cho mỗi chuỗi token\n",
    "    - Minh họa:\n",
    "    \n",
    "        - `Text 1`: \"I love programming.\"\n",
    "        - `Text 2`: \"Python is a powerful programming language.\"\n",
    "\n",
    "    - `Truncation`:\n",
    "\n",
    "        - `Text 1`: [\"I\", \"love\", \"programming\", \".\", \"[SEP]\", \"[CLS]\"] - vì độ dài text = 4 nên khi max_length=5 thì nó không cần phải bỏ bớt từ nào hết.\n",
    "        - `Text 2`: [\"Python\", \"is\", \"a\", \"powerful\", \"programming\"] - text dài hơn 5 nên nó bị cắt thành 5 token\n",
    "        => Nếu văn bản quá dài, token sẽ bị loại bỏ từ cuối để chỉ giữ lại số lượng token tối đa.\n",
    "\n",
    "    - `Padding`:\n",
    "    \n",
    "        - Giả sử chúng ta có một max_length=5 và padding=\"max_length\", tokenizer sẽ đệm các token thiếu bằng ký tự đặc biệt [PAD] để đạt độ dài tối đa\n",
    "        - Ví dụ:\n",
    "            - `text 1`: Sau khi padding, nó sẽ trở thành: [\"I\", \"love\", \"programming\", \".\", \"[PAD]\"]\n",
    "            - `text 2`: Sau khi padding, không cần thêm gì vì nó đã có đủ 5 token.\n",
    "\n",
    "=> `truncation` giúp cắt bớt văn bản quá dài, và `padding` giúp đảm bảo rằng mọi đoạn văn bản đều có cùng độ dài\n",
    "\n",
    "`.map` - áp dụng hàm tokenize cho mỗi phần tử trong dataset, `batched=True` sẽ giúp xử lý các ví dụ theo nhóm => giúp tăng tốc quá trình "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tinh chỉnh trên một phần nhỏ của toàn bộ tập dữ liệu giúp giảm thời gian thực hiện. Kết quả có thể không tốt bằng khi tinh chỉnh trên toàn bộ tập dữ liệu, nhưng cách này sẽ giúp bạn đảm bảo mọi thứ hoạt động đúng trước khi quyết định đào tạo trên toàn bộ dữ liệu.\n",
    "\n",
    "=> đây là phép thử để kiểm tra cách hoạt động của models có giống với kì vọng không mà không làm tốn quá nhiều thời gian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval = dataset[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2`. `Trainer`\n",
    "\n",
    "Là một vòng lặp đào tạo được tối ưu hóa cho các mô hình Transformers, giúp chúng ta dễ dàng bắt đầu đào tạo ngay mà không cần phải tự viết mã đào tạo lại.\n",
    "- Chọn và lựa chọn từ nhiều tính năng đào tạo trong TrainingArguments như tích lũy gradient, độ chính xác hỗn hợp và các tùy chọn để báo cáo và ghi nhật ký số liệu đào tạo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:07:27.314480: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-11 01:07:27.323254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744308447.333900   70899 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744308447.337051   70899 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744308447.345509   70899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744308447.345520   70899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744308447.345522   70899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744308447.345523   70899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-11 01:07:27.348720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bạn có thể nên HUẤN LUYỆN mô hình này trên một tác vụ hạ nguồn để có thể sử dụng nó cho các dự đoán và suy luận.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n",
    "\"Một số trọng số của BertForSequenceClassification không được khởi tạo từ điểm kiểm tra mô hình tại google-bert/bert-base-cased và được khởi tạo mới: ['classifier.bias', 'classifier.weight']\" \n",
    "\"Bạn có thể nên HUẤN LUYỆN mô hình này trên một tác vụ hạ nguồn để có thể sử dụng nó cho các dự đoán và suy luận.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_labels=5` sẽ thay đổi lớp cuối cùng thành tác vụ phân loại với 5 nhãn khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`Note`:Thông báo ở trên là lời nhắc nhở rằng đầu được đào tạo trước của mô hình bị loại bỏ và thay thế bằng đầu phân loại được khởi tạo ngẫu nhiên. Đầu được khởi tạo ngẫu nhiên cần được tinh chỉnh trên tác vụ cụ thể của bạn để đưa ra các dự đoán có ý nghĩa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với mô hình đã tải, ta có thể thiết lập `hyperparameter` trong TrainingArguments. \n",
    "- `Hyperparameter` - là các biến kiểm soát quá trình traning (lr, size_batch, epoch) từ đó ảnh hưởng đến hiệu suất của mô hình. \n",
    "- Việc chọn hyperparameters rất quan trọng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer` yêu cầu một hàm để tính toán và báo cáo số liệu. Đối với tác vụ phân loại, ta sẽ dùng evaluate.load để tải hàm `Evaluate` từ thư viện Evaluate. Sau đó thu thập các dự đoán và nhãn compute để tính toán evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\") # lệnh tải metrics accuracy\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred # tuple chứa logits và labels\n",
    "    \n",
    "    predictions = np.argmax(logits, axis=-1) # Lấy giá trị lớn nhất trong logits dọc theo chiều cuối cùng\n",
    "    return metric.compute(predictions=predictions, references=labels) # sau khi có pred thì nó sẽ tính acc bằng cách so sánh chúng với nhãn thực tế"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập TrainingArguments với nơi save models và thời điểm tính acc\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"yelp_review_classifier\", # thư mục chứa model\n",
    "    eval_strategy=\"epoch\", # thời gian tính accuracy\n",
    "    push_to_hub=True, # tải models lên Hub sau khi train\n",
    "    # Thêm 2 dòng dưới vào để giảm batch size để không bị tràn bộ nhớ\n",
    "    per_device_train_batch_size=8,  # Giảm kích thước batch size\n",
    "    per_device_eval_batch_size=8,   # Giảm kích thước batch size cho eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41' max='243750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    41/243750 00:10 < 18:25:58, 3.67 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Khởi tạo một Trainer và truyền cho nó model, đối số đào tạo, tập train, test\u001b[39;00m\n\u001b[32m      3\u001b[39m trainer = Trainer(\n\u001b[32m      4\u001b[39m     model=model,\n\u001b[32m      5\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     compute_metrics=compute_metrics, \n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# gọi đào tạo\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/python/python_programming/pytorch/.venv/lib/python3.12/site-packages/transformers/trainer.py:2236\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2233\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2234\u001b[39m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[32m   2235\u001b[39m     hf_hub_utils.disable_progress_bars()\n\u001b[32m-> \u001b[39m\u001b[32m2236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2237\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2243\u001b[39m     hf_hub_utils.enable_progress_bars()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/python/python_programming/pytorch/.venv/lib/python3.12/site-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/python/python_programming/pytorch/.venv/lib/python3.12/site-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/python/python_programming/pytorch/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:575\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m         current_batch = \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m    577\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/python/python_programming/pytorch/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:179\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    176\u001b[39m         skip_keys = []\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[32m    178\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor.items()\n\u001b[32m    181\u001b[39m         }\n\u001b[32m    182\u001b[39m     )\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/python/python_programming/pytorch/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:153\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    151\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mnpu:0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "# Khởi tạo một Trainer và truyền cho nó model, đối số đào tạo, tập train, test\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "trainer.train() # gọi đào tạo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiện tại tôi đang áp dụng 2 cách để giảm thiểu việc sử dụng bộ nhớ GPU và tránh lỗi OutOfMemoryError\n",
    "- Tôi giảm kích thước batch size => giảm tải bộ nhớ GPU\n",
    "- Giảm độ dài chuỗi đầu vào (sequence length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TensorFlow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer không tương thích với các mô hình Trainsformers TensorFlow nên ta thay bằng Keras vì các mô hình Transformer được triển khai dưới dạng `tf.keras.Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TFAutoModelForSequenceClassification\n",
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "# def tokenize(examples):\n",
    "#     return tokenizer(examples[\"text\"])\n",
    "\n",
    "# dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.Dataset` - một lớp trong TensorFlow (tf) giúp xử lý và quản lý dữ liệu hiệu quả khi huấn luyện mô hình học sâu. Nó cung cấp các công cụ để tạo ra các pipeline dữ liệu (data pipeline) có thể lặp lại, dễ dàng phân tán, và có thể tối ưu hóa cho việc huấn luyện mô hình. `datapipeline` này chủ yếu để transform data trước khi đưa vào dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có hai phương pháp để chuyển đổi một tập dữ liệu thành `tf.data.Dataset`\n",
    "\n",
    "- `prepare_tf_dataset()` là cách được khuyến nghị để tạo tf.data.Dataset vì ta có thể kiểm tra mô hình để tìm ra cột nào sẽ sử dụng làm đầu vào và cột nào sẽ loại bỏ => tạo ra dataset đơn giản, hiệu suất cao\n",
    "\n",
    "- `to_tf_dataset` là phương thức cấp thấp hơn trong thư viện Datasets cho phép kiểm soát tốt hơn cách tạo tập dữ liệu bằng cách chỉ định các cột và nhãn cột cần sử dụng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thêm `tokenize` vào để `prepare_tf_dataset()` để padding cho từng batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong TensorFlow và PyTorch, việc xử lý dữ liệu (như tokenization, padding, augmentation, v.v.) có thể được thực hiện qua một hàm `collate_fn` (trong PyTorch).\n",
    "\n",
    "Trong PyTorch, `collate_fn` là một hàm tuỳ chỉnh dùng để kết hợp các phần tử của một batch và xử lý chúng trước khi truyền vào mô hình.\n",
    "- Hàm này rất hữu ích khi bạn cần thực hiện những bước xử lý phức tạp như padding không đều hoặc xử lý dữ liệu đặc biệt trước khi đưa vào mô hình\n",
    "- `Uneven padding` - Là việc sử dụng padding linh hoạt cho mỗi chuỗi, sao cho mỗi chuỗi được đệm đến độ dài tối đa của chính nó thay vì một giá trị cố định."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_dataset = model.prepare_tf_dataset(\n",
    "#     dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Biên dịch và điều chỉnh để bắt đầu train\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# model.compile(optimizer=Adam(3e-5))\n",
    "# model.fit(tf_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
