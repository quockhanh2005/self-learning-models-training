{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 36px; color: #FFD700\">2 - A Gentle Introduction to torch.autograd</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BACKGROUND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network - là tập hợp các hàm lồng nhau đc thực thi trên một số dữ liệu đầu vào. Các hàm này đc xác định bởi các tham số (weight & bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation: Trong quá trình này, NN sẽ đưa ra dự đoán tốt nhất về đầu ra dựa vào những gì nó đã học được cho đến thời điểm đó.\n",
    "\n",
    "=> nhìn đầu vào -> dự đoán đầu ra trên trọng số hiện tại \n",
    "\n",
    "=> cố gắng dự đoán đúng nhất có thể với cái mà nó chưa biết đúng sai (theo niềm tin của nó)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Propagation: NN điều chỉnh các tham số của nó theo tỷ lệ với lỗi trong dự đoán của nó. Nó thực hiện bằng cách duyệt ngược từ đầu ra, thu thập các đạo hàm của lỗi liên quan đến các tham số của các hàm gradient và tối ưu jpas bằng cách dùng gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ:\n",
    "- Dùng mô hình ResNet18 đã được huấn luyện sẵn từ thư viện torchvision\n",
    "- Tạo dữ liệu đầu vào ngẫu nhiên: 1 ảnh có kích thước 3×64×64 (RGB, cao & rộng là 64)\n",
    "- Gán một label ngẫu nhiên (để giả lập nhãn ảnh)\n",
    "\n",
    "Vì ResNet18 được huấn luyện trên ImageNet, nên đầu ra của nó có dạng (1, 1000) — tức là dự đoán cho 1000 lớp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bước Forward Propagation - Ta chạy input data vào mô hình qua từng lớp để đưa ra dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta dùng pred + label tương ứng đế tính loss. Tiếp theo là backward qua mạng. \n",
    "\n",
    "Truyền ngược bắt đầu khi ta gọi tensor loss.backward() sau đó Autograd tính toán và lưu trữ các grad cho từng tham số cho mô hình trong thuộc tính .grad của tham số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sẽ tải một optimizer (SGD) với lr = 0.01, momentum=0.9 ta truyền tất cả các tham số của mô hình trong trình tối ưu hóa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó ta gọi .step() để bắt đầu gradient descent. Optimizer điều chỉnh từng tham số theo grad được lưu trữ trong .grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta tạo ra 2 tensor a và b với requires_grad=True (mọi hđ trên autograd đều phải theo dõi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4,], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta tạo ra một tensor Q từ a và b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3 * a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử a và b là các tham số của NN và Q là loss. Trong qtrinh train, ta muốn có các grad của loss theo các tham số \n",
    "\n",
    "$\\frac{\\partial Q}{\\partial a} = 9a^2$\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial b} = -2b$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi ta gọi Q.backward, autograd sẽ tính toán các grad và lưu trữ trong trong thuộc tính grad tensor tương ứng. \n",
    "\n",
    "Ta cần truyền một gradient đối số một cách rõ ràng Q.backward() vì nó là một vector. gradient là một tensor có cùng hình dạng với Q và nó biểu diễn độ dốc của Q so với chính nó, tức là $\\frac{dQ}{dQ} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu Q là scalar thì không cần truyền gì và ngược alij thì truyền thêm gradient vào.\n",
    "- Tensor gradient truyền vào có cùng shape với Q\n",
    "- Mục đích truyền vào để xác định grad theo phần tử nào"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tương đương, chúng ta cũng có thể tổng hợp Q thành một số vô hướng và gọi ngược lại một cách ngầm định, như Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các gradient hiện được gửi vào a.grad và b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VECTOR CALCULUS USING AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu ta có một hàm vector $\\vec{y} = f(\\vec{x})$ thì gradient của $\\vec{y}$ theo $vec{x}$ là ma trận Jacobian J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J = \\left( \\frac{\\partial \\vec{y}}{\\partial x_1} \\ \\cdots \\ \\frac{\\partial \\vec{y}}{\\partial x_n} \\right)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd là công cụ để tính tích vector Jacobian - nghĩa là bất cứ vector $\\vec{v}$ nào tính đều tính tích $J^T \\cdot \\vec{v}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu $\\vec{v}$ tình cờ là gradient của một hàm vô hướng $l = g(\\vec{y}):$ thì\n",
    "\n",
    "$$\n",
    "\\vec{v} = \\left( \\frac{\\partial l}{\\partial y_1} \\ \\cdots \\ \\frac{\\partial l}{\\partial y_m} \\right)^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hàm vô hướng: chỉ nhận một giá trị đầu ra (scala). Vd hàm loss nhận một vector $\\vec{y}$ và trả về một sô thực l.\n",
    "- Gradient của hàm vô hướng đó theo $\\vec{y}$:\n",
    "    - Gradient ở đây chính là vector đạo hàm riêng của l theo từng phần của $\\vec{y}$ ký hiệu: $\\vec{v} = \\nabla_{\\vec{y}} \\, l$\n",
    "    - Nghĩa là: \n",
    "    $$\n",
    "    \\vec{v} = \\left[ \\frac{\\partial l}{\\partial y_1}, \\ \\frac{\\partial l}{\\partial y_2}, \\ \\cdots, \\ \\frac{\\partial l}{\\partial y_m} \\right]\n",
    "    $$\n",
    "- Ý nghĩa trong chain rule:\n",
    "    - Nếu $\\vec{y} = f(\\vec{x})$, tức $\\vec{y}$ phụ thuộc vào $\\vec{x}$, thì gradient của l theo $\\vec{x}$ là: $\\nabla_{\\vec{x}} \\, l = J^T \\cdot \\vec{v}$\n",
    "    - Trong đó J là ma trận  Jacobian của $\\vec{y}$ theo $\\vec{x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Chain rule) -Tích vector Jacobian là gradient của l theo $\\vec{x} là:\n",
    "$$\n",
    "J^T \\cdot \\vec{v} =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial \\text{v\\_đi}_1}{\\partial x_1} & \\cdots & \\frac{\\partial \\text{v\\_đi}_i}{\\partial x_1} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\text{v\\_đi}_1}{\\partial x_N} & \\cdots & \\frac{\\partial \\text{v\\_đi}_i}{\\partial x_N}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial \\text{một}}{\\partial \\text{v\\_đi}_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\text{một}}{\\partial \\text{v\\_đi}_i}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial \\text{một}}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\text{một}}{\\partial x_N}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPUTATIONAL GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd lưu trữ một bản ghi dữ liệu (tensor) và tất cả các hoạt động được thực hiện cùng với các tensor kết quả trong một đồ thị có hướng không có chu trình (DAG) bao gồm các function obj. \n",
    "\n",
    "Trong DAG:\n",
    "- lá là các input tensor, gốc là output tensor. \n",
    "- theo dõi từ gốc đến lá => có thể tự động tính toán gradient bằng các sử dụng chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd thực hiện hai việc cùng lúc trong mỗi lần chuyển tiếp:\n",
    "- chạy hoạt động được yêu cầu tính toán một tensor kết quả\n",
    "- duy trì hàm grad của hoạt động đó trog DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation bắt đầu khi gọi .backward() ở gốc DAG. Sau đó:\n",
    "- Tính các gradient từ mỗi .grad_fn\n",
    "- tích lũy các giá trị gradient trong thuộc tính tensor .grad\n",
    "- sử dụng quy tắc chuỗi, lan truyền đến các leaf-tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biểu diễn trực quan của DAG:\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/dag_autograd.png\" alt=\"Ảnh online\" style=\"background-color:white; padding:10px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giải thích:\n",
    "- Hai ô màu xanh dương là input tensor\n",
    "- Theo hương mũi tên là Forward \n",
    "- Các ô màu xám là các phép toán\n",
    "- Ô màu xanh là phép trừ với đầu vào là từ hai nhánh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ:\n",
    "Ở ô xanh dương đầu tiền có 2 biến đầu vào là x a\n",
    "\n",
    "Ở ô màu xanh dương thứ 2 có biến đầu vào là y\n",
    "\n",
    "Cột trái:\n",
    "- PowBackward(): $x^2$\n",
    "- MulBackward(): a * $x^2$\n",
    "\n",
    "Cột phải:\n",
    "- PowBackward(): $y^2$\n",
    "\n",
    "SubBackward(): a * $x^2$ - $y^2$\n",
    "\n",
    "=> Đây là các Pytorch xây dựng đồ thì để tính toán grad cho z với từng input\n",
    "\n",
    "Khi gọi backward() - z.backward()\n",
    "- Bắt đầu từ SubBackward() - cuối đồ thị \n",
    "- Quay lai từng node và tự áp dụng quy tắt đạo hàm tương tự.\n",
    "- Cập nhật .grad cho từng tensor input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Việc lưu trữ grad là để cập nhật trong bước optimizer.step(). Còn bước forward hay backward là để tính toán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclusion from the DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd sẽ theo dõi các hoạt động tất cả tensor có requires_grad mà cờ = True. Ngược lại (False) sẽ loại tensor đó ra khỏi DAG tính gradient \n",
    "\n",
    "output tensỏ của một phép toán vẫn yêu cầu gradient khi chỉ có một input set requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu ta set requires_grad = False cho tất cả input:\n",
    "    - Mô hình đang ở chế độ prediction\n",
    "    - Nếu ở chế độ train thì không học đc gì hết"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các tham số không tính đạo hàm được gọi là \"frozen parameters\" - hữu ích khi ta biết những tham số không cần đạo hàm \n",
    "\n",
    "=> mang lại lợi ích về hiệu suất khi giảm các phép tính tự động\n",
    "\n",
    "Trong tinh chỉnh models, ta thường sẽ đóng băng hầu hết mô hình, thường chỉ sửa đổi lớp phân loại để đưa ra dự đoán về nhãn mới. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ta tải model resnet18 đã đc train và đóng băng tất cả các tham số\n",
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vd: ta tinh chỉnh mô hình trên một tập huấn luyện vs 10 nhãn. \n",
    "\n",
    "Trong resnet, bộ phân loại là lớp tuyến tính cuối cùng (model.fc) Chúng ta có thể chỉ cần thay thế nó bằng một lớp tuyến tính mới (mặc định là không đóng băng) đóng vai trò là bộ phân loại của chúng ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ các tham số trong mô hình, ngoại trừ các tham số của model.fc, đều bị đóng băng. Các tham số duy nhất tính gradient là weights và bias của model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý: \n",
    "\n",
    "Dù ta đăng ký tất cả các tham số trong optimizer, nhưng các tham số duy nhất tính gradient (và do đó được cập nhật trong quá trình giảm gradients) là weights và bias của trình phân loại."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
