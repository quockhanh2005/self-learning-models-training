{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 36px; color: #FFD700\">7 - OPTIMIZING-MODEL-PARAMETERS</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mục tiêu: Tối ưu tham số của models trên dữ liệu của ta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đào tạo mô hình là một quá trình lặp đi lặp lại; trong mỗi lần lặp lại, mô hình đưa ra dự đoán về đầu ra, tính toán lỗi trong dự đoán của nó ( loss ), thu thập các đạo hàm của lỗi liên quan đến các tham số của nó và tối ưu hóa các tham số này bằng cách sử dụng gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy dữ liệu có sẵn từ pytorch\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tải dữ liệu vào data loader theo batch\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một mạng 3 lớp\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siêu tham số là các tham số có thể điều chỉnh cho phép kiểm soát quá trình tối ưu hóa mô hình. Các giá trị siêu tham số khác nhau có thể tác động đến tốc độ đào tạo và hội tụ của mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xác định các hyperparameter sau để đào tạo:\n",
    "- epochs: số lần lặp lại trên tập dữ liệu\n",
    "- batch_size: số lượng mẫu dữ liệu được truyền qua mạng trước khi các tham số được cập nhật\n",
    "- learning_rate: mức độ cập nhật các tham số mô hình ở mỗi batch/epochs. Các giá trị nhỏ hơn tạo ra tốc độ học chậm, trong khi các giá trị lớn có thể dẫn đến hành vi không thể đoán trước trong quá trình đào tạo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZATION LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi ta thiết lập hyperparameters, ta có thể đào tạo và tối ưu hóa mô hình của mình bằng optimization loop. Mỗi lần lặp lại của optimization loop được gọi là một epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi epoch bao gồm 2 phần chính:\n",
    "- Train loop: lặp lại tập dữ liệu đào tạo và cố gắng hội tụ đến các tham số tối ưu.\n",
    "- Validation/test loop: lặp lại tập dữ liệu kiểm tra để kiểm tra xem hiệu suất mô hình có được cải thiện hay không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi được cung cấp một số dữ liệu đào tạo, mạng chưa được đào tạo có khả năng không đưa ra câu trả lời đúng.\n",
    "\n",
    "=> Hàm mất mát đo lường mức độ không giống nhau của kết quả thu được so với giá trị mục tiêu và đó là hàm mất mát mà ta muốn giảm thiểu trong quá trình đào tạo. Để tính toán mất mát, ta đưa ra dự đoán bằng cách sử dụng các đầu vào của mẫu dữ liệu đã cho và so sánh với giá trị nhãn dữ liệu thực"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các hàm mất mát phổ biến bao gồm nn.MSELoss (Lỗi bình phương trung bình) cho các tác vụ hồi quy và nn.NLLLoss (Khả năng logarit âm) cho phân loại. nn.CrossEntropyLoss kết hợp nn.LogSoftmaxvà nn.NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss() # ta truyền logits đầu ra của models tới nn.CrossEntropyLoss \n",
    "# => chuẩn hóa logits và tính toán lỗi dữ đoán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tối ưu hóa là quá trình điều chỉnh các tham số mô hình để giảm lỗi mô hình trong mỗi bước đào tạo. Tất cả logic tối ưu hóa được đóng gói trong đối tượng optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo bằng cách truyền vào các tham số mô hình cần đc đào tạo và siêu tham số learning_rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong training loop, quá trình tối ưu hóa diễn ra theo ba bước:\n",
    "- Gọi optimizer.zero_grad() để thiết lập lại gradient của các tham số mô hình. Theo mặc định, gradient sẽ cộng dồn; để tránh đếm trùng, ta sẽ đặt chúng về 0 một cách rõ ràng ở mỗi lần lặp lại.\n",
    "- Truyền ngược lại tổn thất dự đoán bằng lệnh gọi đến loss.backward(). PyTorch gửi các gradient của tổn thất liên quan đến từng tham số.\n",
    "- Khi đã có gradient, chúng ta gọi optimizer.step()để điều chỉnh các tham số theo gradient được thu thập trong quá trình truyền ngược."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triển khai đầy đủ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)  # Tổng số mẫu trong tập huấn luyện\n",
    "\n",
    "    model.train()  # Đặt model về chế độ huấn luyện\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Dự đoán và tính loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Lan truyền ngược và cập nhật trọng số\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # In loss mỗi 100 batch\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.item()\n",
    "            current = batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss_value:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Đặt mô hình ở chế độ đánh giá (evaluation mode)\n",
    "    # Giúp tắt dropout, batchnorm hoạt động ổn định hơn\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)      # Tổng số mẫu test\n",
    "    num_batches = len(dataloader)       # Tổng số batch\n",
    "    test_loss, correct = 0, 0           # Biến lưu tổng loss và số mẫu đúng\n",
    "\n",
    "    # Tắt tính gradient để tiết kiệm bộ nhớ và tăng tốc độ\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)                                 # Dự đoán\n",
    "            test_loss += loss_fn(pred, y).item()            # Cộng dồn loss\n",
    "            correct += (pred.argmax(1) == y).float().sum().item()  # Đếm số dự đoán đúng\n",
    "\n",
    "    # Tính loss trung bình và accuracy toàn tập\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta đã tạo Loss_function và optimizer, sau đó truyền nó cho train_loop và test_loop. Ta có thể tăng epoch và batch_size để theo dõi hiệu suất mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.308785  [   64/60000]\n",
      "loss: 2.291500  [ 6464/60000]\n",
      "loss: 2.278466  [12864/60000]\n",
      "loss: 2.272098  [19264/60000]\n",
      "loss: 2.257041  [25664/60000]\n",
      "loss: 2.235273  [32064/60000]\n",
      "loss: 2.240129  [38464/60000]\n",
      "loss: 2.207500  [44864/60000]\n",
      "loss: 2.203838  [51264/60000]\n",
      "loss: 2.176012  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 2.167694 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.181559  [   64/60000]\n",
      "loss: 2.170525  [ 6464/60000]\n",
      "loss: 2.117822  [12864/60000]\n",
      "loss: 2.126220  [19264/60000]\n",
      "loss: 2.085720  [25664/60000]\n",
      "loss: 2.029249  [32064/60000]\n",
      "loss: 2.058931  [38464/60000]\n",
      "loss: 1.978039  [44864/60000]\n",
      "loss: 1.989515  [51264/60000]\n",
      "loss: 1.918452  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.912824 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.949598  [   64/60000]\n",
      "loss: 1.920287  [ 6464/60000]\n",
      "loss: 1.806952  [12864/60000]\n",
      "loss: 1.837857  [19264/60000]\n",
      "loss: 1.739152  [25664/60000]\n",
      "loss: 1.688069  [32064/60000]\n",
      "loss: 1.714521  [38464/60000]\n",
      "loss: 1.605609  [44864/60000]\n",
      "loss: 1.637944  [51264/60000]\n",
      "loss: 1.532908  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.546361 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.612626  [   64/60000]\n",
      "loss: 1.578229  [ 6464/60000]\n",
      "loss: 1.430496  [12864/60000]\n",
      "loss: 1.494106  [19264/60000]\n",
      "loss: 1.383730  [25664/60000]\n",
      "loss: 1.374789  [32064/60000]\n",
      "loss: 1.392262  [38464/60000]\n",
      "loss: 1.305286  [44864/60000]\n",
      "loss: 1.348012  [51264/60000]\n",
      "loss: 1.246649  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.271372 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.347144  [   64/60000]\n",
      "loss: 1.329417  [ 6464/60000]\n",
      "loss: 1.167624  [12864/60000]\n",
      "loss: 1.263859  [19264/60000]\n",
      "loss: 1.147935  [25664/60000]\n",
      "loss: 1.169067  [32064/60000]\n",
      "loss: 1.190761  [38464/60000]\n",
      "loss: 1.118696  [44864/60000]\n",
      "loss: 1.166246  [51264/60000]\n",
      "loss: 1.078787  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.099708 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.169571  [   64/60000]\n",
      "loss: 1.172228  [ 6464/60000]\n",
      "loss: 0.995136  [12864/60000]\n",
      "loss: 1.119456  [19264/60000]\n",
      "loss: 1.003354  [25664/60000]\n",
      "loss: 1.030914  [32064/60000]\n",
      "loss: 1.067180  [38464/60000]\n",
      "loss: 1.000116  [44864/60000]\n",
      "loss: 1.048759  [51264/60000]\n",
      "loss: 0.974196  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.989332 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.047250  [   64/60000]\n",
      "loss: 1.071075  [ 6464/60000]\n",
      "loss: 0.877136  [12864/60000]\n",
      "loss: 1.024002  [19264/60000]\n",
      "loss: 0.912634  [25664/60000]\n",
      "loss: 0.933655  [32064/60000]\n",
      "loss: 0.987079  [38464/60000]\n",
      "loss: 0.922711  [44864/60000]\n",
      "loss: 0.967707  [51264/60000]\n",
      "loss: 0.904073  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.914289 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.957946  [   64/60000]\n",
      "loss: 1.001191  [ 6464/60000]\n",
      "loss: 0.792380  [12864/60000]\n",
      "loss: 0.956758  [19264/60000]\n",
      "loss: 0.851883  [25664/60000]\n",
      "loss: 0.862346  [32064/60000]\n",
      "loss: 0.931331  [38464/60000]\n",
      "loss: 0.870776  [44864/60000]\n",
      "loss: 0.909099  [51264/60000]\n",
      "loss: 0.853174  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.860211 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.889395  [   64/60000]\n",
      "loss: 0.948759  [ 6464/60000]\n",
      "loss: 0.728733  [12864/60000]\n",
      "loss: 0.906378  [19264/60000]\n",
      "loss: 0.808520  [25664/60000]\n",
      "loss: 0.808511  [32064/60000]\n",
      "loss: 0.889353  [38464/60000]\n",
      "loss: 0.834435  [44864/60000]\n",
      "loss: 0.865166  [51264/60000]\n",
      "loss: 0.813977  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.819351 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.834630  [   64/60000]\n",
      "loss: 0.906961  [ 6464/60000]\n",
      "loss: 0.679220  [12864/60000]\n",
      "loss: 0.867374  [19264/60000]\n",
      "loss: 0.775479  [25664/60000]\n",
      "loss: 0.767194  [32064/60000]\n",
      "loss: 0.855612  [38464/60000]\n",
      "loss: 0.807926  [44864/60000]\n",
      "loss: 0.831136  [51264/60000]\n",
      "loss: 0.782533  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.787082 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # Hàm mất mát dùng cho phân loại nhiều lớp\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # Tối ưu SGD\n",
    "\n",
    "epochs = 10  # Số lần huấn luyện toàn bộ dữ liệu (epoch)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    # Huấn luyện model với dữ liệu huấn luyện\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # Đánh giá model với dữ liệu kiểm tra\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")  # In ra khi huấn luyện xong"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
