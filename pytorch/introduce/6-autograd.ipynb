{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 36px; color: #FFD700\">6 - AUTOGRAD</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thuật toán được sử dụng nhiều khi huấn luyện mạng neural là lan truyền ngược. Trong thuật toán này, các tham số - weight đc điều chỉnh theo độ dốc của hàm loss với tham số đã cho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd để tính toán gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([1., 1., 1., 1., 1.])\n",
      "output: tensor([0., 0., 0.])\n",
      "weight: tensor([[-0.0051, -0.3425, -0.6627],\n",
      "        [ 0.3125, -0.3847,  1.8364],\n",
      "        [-0.3172,  0.0888, -1.0711],\n",
      "        [ 1.3015,  1.0996, -0.8059],\n",
      "        [ 0.2707,  0.2941, -0.1797]], requires_grad=True)\n",
      "bias: tensor([ 1.4978, -0.0189, -0.6739], requires_grad=True)\n",
      "output_pred: tensor([ 3.0602,  0.7364, -1.5569], grad_fn=<AddBackward0>)\n",
      "loss: 1.4749889373779297\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # một tensor đầu vào với 5 phần tử\n",
    "print(f\"input: {x}\") \n",
    "y = torch.zeros(3) # output kì vọng\n",
    "print(f\"output: {y}\")\n",
    "w = torch.randn(5, 3, requires_grad= True) # ma trận trọng số\n",
    "print(f\"weight: {w}\")\n",
    "b = torch.randn(3, requires_grad= True) # bias\n",
    "print(f\"bias: {b}\")\n",
    "z = torch.matmul(x, w) + b # phương trình tìm z với x đầu vào và trọng số\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(f\"output_pred: {z}\")\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta có thể thiết laappj requires_grad khi tạo tensor hoặc sử dụng method x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đồ thị tính toán:\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" alt=\"Ảnh có nền trắng\" style=\"background-color: white;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w, b là các parameter cần tối ứu => phải đặt requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thuộc tính - grad_fn, ta áp dụng cho tensor là một đối tượng của lớp Function - trỏ đến một đối tượng của một lớp con kế thừa từ torch.autograd.Function. \n",
    "\n",
    "- đối tượng này là hàm(matmul, +) đã tạo ra tensor khi lan truyền tiến - forward\n",
    "\n",
    "- Nó có chứa logic để thực hiện backward\n",
    "\n",
    "=> grad_fn lưu trữ đối tượng mô ta các phép toán hình thành nên tensor đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x70342df602e0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x70342795e980>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tối ưu, ta tính gradient của loss function của chúng ta với các parameter. Cụ thể ta cần\n",
    "$\\frac{\\partial \\text{loss}}{\\partial w}$\n",
    "$\\frac{\\partial \\text{loss}}{\\partial b}$\n",
    "của một số giá trị cố định của x và y\n",
    "\n",
    "=> để tính ta gọi loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3184, 0.2254, 0.0580],\n",
      "        [0.3184, 0.2254, 0.0580],\n",
      "        [0.3184, 0.2254, 0.0580],\n",
      "        [0.3184, 0.2254, 0.0580],\n",
      "        [0.3184, 0.2254, 0.0580]])\n",
      "tensor([0.3184, 0.2254, 0.0580])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leaf_node + requires_grad=True => tự động có .grad sau khi gọi loss.backward. Còn lại gradient không khả dụng \n",
    "- leaf_node: Là những tensor do ta tạo ra, không phải kq của phép toán khác. Vd: x, y, w, b\n",
    "\n",
    "ta chỉ có thể thực hiện grad bằng cách dùng backward một lần trên một đồ thị nhất định vì hiệu suất. \n",
    "- retain_graph=True => thực hiện nhiều backward trên cùng một đồ thị "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vô hiệu hóa theo dõi gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi tạo tensor vs requires_grad=True:\n",
    "- Theo dõi tất cả các phép toán ta thực hiện trên tensor đó\n",
    "- Dùng để xây dựng đồ thị tính toán => phục vụ Backward\n",
    "\n",
    "Khi không cần gradient\n",
    "- Mô hình train xong\n",
    "- Chỉ muốn dự đoán đầu ra từ dữ liệu mới\n",
    "\n",
    "=> theo dõi phép toán và tính gradient là vô ích => tốn resource\n",
    "\n",
    "===> Bao quanh mã tính toán bằng block torch.no_grad() / detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# bọc quanh mã tính toán bằng block torch.no_grad()\n",
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# detach()\n",
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lý do khác nên tắt tính năng theo dõi grad:\n",
    "- Để đánh dấu một số tham số trong mạng neural là tham số cố định => khi chỉ muốn fine-tuning pretrained-model, đóng băng một số lớp\n",
    "- Các phép tính trên tensor khi không theo dõi grad sẽ hiệu quả hơn => tăng tốc độ tính toán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd là hệ thống tự động tính đạo hàm, khi đó:\n",
    "- Thực hiện phép tính như bình thường → ví dụ: nhân ma trận, cộng, sigmoid, loss, v.v\n",
    "- Ghi lại các phép toán đó vào một đồ thị có hướng (DAG)\n",
    "    - Mỗi phép toán là một nút Function trong đồ thị\n",
    "    - Mỗi tensor trung gian nhớ lại phép toán đã tạo ra nó thông qua .grad_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cấu trúc đồ thị:\n",
    "- Leaf: là các tensor đầu vào - ta tạo ra: x, w, b\n",
    "- Root: là tensor đầu ra cuối cùng, thường là loss\n",
    "\n",
    "Đồ thị DAG => không có vòng lặp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi gọi .backward() tại root(loss.backward()), khi đó:\n",
    "1. Bắt đầu từ loss - gốc của DAG\n",
    "2. Tính grad của từng phép toán bằng .grad_fn\n",
    "3. Tích lũy grad vào thuộc tính .grad của các tensor có requires_grad=True\n",
    "4. Lan truyền ngược xuống tới các tensor lá bằng quy chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ghi chú: \n",
    "\n",
    "DAG là dynamic - sau mỗi lần gọi .backward, autograd tự xây lại một đồ thị mới. Sau khi backward xong thì đồ thị cũ bị xóa. Đây là điều cho phép ta sử dụng câu lệnh điều khiển luồng (if, for, while) trong môt hình. Ta có thể thay đổi shape, size, operator ở mỗi lần lặp nếu cần mà autograd vẫn đoạt động chính xác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Lần 1 ---\n",
      "Hàm sử dụng: x * 2\n",
      "x.grad = tensor([2.])\n",
      "\n",
      "--- Lần 2 ---\n",
      "Hàm sử dụng: x ** 2\n",
      "x.grad = tensor([4.])\n",
      "\n",
      "--- Lần 3 ---\n",
      "Hàm sử dụng: x * 2\n",
      "x.grad = tensor([2.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    if i % 2 == 0:\n",
    "        y = x * 2      # Lần 1 & 3: phép nhân\n",
    "    else:\n",
    "        y = x ** 2     # Lần 2: phép lũy thừa\n",
    "\n",
    "    # nếu DAG tĩnh => lỗi hoặc kq không thay đổi\n",
    "    print(f\"--- Lần {i+1} ---\")\n",
    "    print(f\"Hàm sử dụng: {'x * 2' if i % 2 == 0 else 'x ** 2'}\")\n",
    "    \n",
    "    y.backward()\n",
    "    print(f\"x.grad = {x.grad}\\n\")\n",
    "    \n",
    "    x.grad.zero_()  # Reset gradient để lặp tiếp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta thấy lần 2 thì kết quả grad vẫn đúng với bản chất grad chứ không giống như kq của lần 1 => đồ thị sẽ cập nhật sau sau mỗi lần gọi backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor gradients and Jacobian Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong trường hợp ta có một hàm loss vô hướng và ta cần tính toán gradient theo một số tham số, có những trường hợp hàm đầu ra là một tensor tùy ý thì Pytorch cho ta tính toán tích Jacobian chứ không phải grad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{Với một hàm vector } \\vec{y} = f(\\vec{x}), \\text{ trong đó } \\vec{x} = \\langle x_1, \\dots, x_n \\rangle, \\vec{y} = \\langle y_1, \\dots, y_m \\rangle, \\text{ gradient của } \\vec{y} \\text{ theo } \\vec{x} \\text{ được cho bởi ma trận Jacobian:} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J = \\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thay vì tính toán ma trận Jacobian, Pytorch cho phép tính toán Jacobian product $\\vec{v}^T \\cdot J$ cho một vector đầu vào nhất định $\\vec{v} = \\langle v_1, \\dots, v_n \\rangle$. Điều này đạt được bằng cách gọi .backward(v). shape v phải giống shape tensor ban đầu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp + 1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi ta gọi backward lần thứ hai với cùng một đối số, giá trị của grad sẽ khác vì khi backward lan truyền, Pytorch tích lũy các grad - giá trị các grad được thêm vào grad thuộc  tính của tất cả các nút lá của đồ thị thuật toán. Nếu muốn tính toán grad thích hợp thì cần phải xóa thuộc tính grad trước. Thực tế thì optimizer sẽ giúp ta thực hiện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ghi chú: Trước đây, backward không có tham số. Điều này tương đương backward(torch.tensor(1.0)) đây hữ ích khi tính grad trong trường hợp hàm có giá trị vô hướng. Vd như loss trong qtrinh train neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
